services:
  # =============================================================================
  # BASE IMAGE SERVICE
  # =============================================================================
  
  ghost-base:
    build:
      context: .
      dockerfile: Dockerfiles/Dockerfile.base
      target: dependencies
      cache_from:
        - ghost-base:dependencies
    image: ghost-base:dependencies
    command: /bin/true
    networks:
      - ghost-network

  # =============================================================================
  # INFRASTRUCTURE SERVICES
  # =============================================================================

  redis:
    image: redis:7-alpine
    container_name: ghost-redis
    ports:
      - "${REDIS_PORT:-6379}:6379"
    volumes:
      - redis-data:/data
    command: redis-server --appendonly yes --appendfsync everysec
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - ghost-network
    restart: unless-stopped

  # =============================================================================
  # SLM Server - Local Small Language Model Inference
  # =============================================================================
  # Provides fast, FREE inference for simple tasks using Phi-3 Mini (3.8B params)
  # Benefits: 70% cost reduction, 5-10x faster for simple tasks
  # Requirements: 4-6GB RAM, ~2.3GB model download

  
  
  slm-server:
    image: ghcr.io/ggerganov/llama.cpp:server
    container_name: ghost-slm-server
    ports:
      - "${SLM_PORT:-8080}:8080"
    volumes:
      - ./models:/models:ro
      - slm-cache:/cache
    # The :server image automatically runs the server
    # Just pass the model path and options as arguments
    command: 
      - "-m"
      - "/models/Phi-3-mini-4k-instruct-q4.gguf"
      - "--host"
      - "0.0.0.0"
      - "--port"
      - "8080"
      - "-c"
      - "4096"
      - "-t"
      - "8"
      - "--metrics"
    networks:
      - ghost-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      start_period: 120s
      retries: 5


  # =============================================================================
  # MCP SERVERS (Tool providers)
  # =============================================================================

  mcp-filesystem:
    build:
      context: .
      dockerfile: Dockerfiles/Dockerfile.mcp
    container_name: ghost-mcp-filesystem
    environment:
      # MCP Configuration
      MCP_SERVER_TYPE: filesystem
      MCP_ALLOWED_DIRECTORIES: ${MCP_ALLOWED_DIRECTORIES:-/app/data}
      MCP_READONLY: ${MCP_READONLY:-false}
      
      # Claude API (for MCP server that needs it)
      ANTHROPIC_API_KEY: ${ANTHROPIC_API_KEY}
      
      # Logging
      LOG_LEVEL: ${LOG_LEVEL:-INFO}
      LOG_FORMAT: ${LOG_FORMAT:-json}
      PYTHONPATH: /app
      PYTHONUNBUFFERED: 1
    volumes:
      - ./common:/app/common:ro
      - ./mcp_server:/app/mcp_server:ro
      - ghost-data:/app/data
      - ghost-logs:/app/logs
    depends_on:
      ghost-base:
        condition: service_completed_successfully
    networks:
      - ghost-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "python", "-c", "import sys; sys.exit(0)"]
      interval: 30s
      timeout: 10s
      start_period: 30s
      retries: 3

  mcp-agents:
    build:
      context: .
      dockerfile: Dockerfiles/Dockerfile.mcp
    container_name: ghost-mcp-agents
    environment:
      # MCP Configuration
      MCP_SERVER_TYPE: agents
      
      # Claude API (for MCP server that needs it)
      ANTHROPIC_API_KEY: ${ANTHROPIC_API_KEY}
      
      # Logging
      LOG_LEVEL: ${LOG_LEVEL:-INFO}
      LOG_FORMAT: ${LOG_FORMAT:-json}
      PYTHONPATH: /app
      PYTHONUNBUFFERED: 1
    volumes:
      - ./common:/app/common:ro
      - ./ghosts:/app/ghosts:ro
      - ./mcp_server:/app/mcp_server:ro
      - ghost-logs:/app/logs
    depends_on:
      ghost-base:
        condition: service_completed_successfully
    networks:
      - ghost-network
    restart: unless-stopped
    command: ["/app/.venv/bin/python", "-m", "mcp_server.agents"]
    healthcheck:
      test: ["CMD", "python", "-c", "import sys; sys.exit(0)"]
      interval: 30s
      timeout: 10s
      start_period: 30s
      retries: 3

  # =============================================================================
  # ORCHESTRATOR
  # =============================================================================

  orchestrator:
    build:
      context: .
      dockerfile: Dockerfiles/Dockerfile.orchestrator
    container_name: ghost-orchestrator
    ports:
      # Orchestrator runs its own A2A server on this port
      - "${ORCHESTRATOR_A2A_PORT:-8765}:8765"
    environment:
      # Agent Configuration
      AGENT_ID: orchestrator
      AGENT_ROLE: orchestrator
      A2A_PORT: 8765
      
      # Redis for service discovery
      REDIS_URL: ${REDIS_URL:-redis://redis:6379}
      
      # Claude API (LLM)
      ANTHROPIC_API_KEY: ${ANTHROPIC_API_KEY}
      LLM_MODEL: ${LLM_MODEL:-claude-sonnet-4-5-20250929}
      
      # SLM Configuration
      SLM_ENDPOINT: ${SLM_ENDPOINT:-http://slm-server:8080}
      SLM_MODEL: ${SLM_MODEL:-phi-3-mini}
      SLM_PROVIDER: ${SLM_PROVIDER:-llama_cpp}
      ENABLE_SLM_FALLBACK: ${ENABLE_SLM_FALLBACK:-true}
      
      # TaskMaster Classification Thresholds
      SIMPLE_THRESHOLD: ${SIMPLE_THRESHOLD:-0.3}
      COMPLEX_THRESHOLD: ${COMPLEX_THRESHOLD:-0.7}
      
      # Orchestrator Settings
      MAX_WORKERS: ${MAX_WORKERS:-10}
      WORKER_TIMEOUT: ${WORKER_TIMEOUT:-300}
      
      # Logging
      LOG_LEVEL: ${LOG_LEVEL:-INFO}
      LOG_FORMAT: ${LOG_FORMAT:-json}
      PYTHONPATH: /app
      PYTHONUNBUFFERED: 1
      ENVIRONMENT: ${ENVIRONMENT:-development}
    volumes:
      - ./common:/app/common:ro
      - ./ghosts:/app/ghosts:ro
      - ./mcp_server:/app/mcp_server:ro
      - ghost-logs:/app/logs
      - ghost-data:/app/data
    depends_on:
      ghost-base:
        condition: service_completed_successfully
      redis:
        condition: service_healthy
    networks:
      - ghost-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "python", "-c", "import sys; sys.exit(0)"]
      interval: 30s
      timeout: 10s
      start_period: 60s
      retries: 3

  # =============================================================================
  # WORKER AGENTS (Scale these as needed)
  # =============================================================================

  worker-1:
    build:
      context: .
      dockerfile: Dockerfiles/Dockerfile.worker
    container_name: ghost-worker-1
    ports:
      # Each worker exposes its own A2A port
      - "${WORKER_1_A2A_PORT:-8766}:8766"
    environment:
      # Agent Configuration (ADDED AGENT_ID)
      AGENT_ID: ${AGENT_ID:-worker-1}
      AGENT_ROLE: worker
      A2A_PORT: 8766
      WORKER_CAPABILITIES: ${WORKER_1_CAPABILITIES:-llm_inference,data_processing,analysis}
      
      # Redis for service discovery
      REDIS_URL: ${REDIS_URL:-redis://redis:6379}
      
      # Claude API (LLM)
      ANTHROPIC_API_KEY: ${ANTHROPIC_API_KEY}
      LLM_MODEL: ${LLM_MODEL:-claude-sonnet-4-5-20250929}
      
      # SLM Configuration (ADDED)
      SLM_ENDPOINT: ${SLM_ENDPOINT:-http://slm-server:8080}
      SLM_MODEL: ${SLM_MODEL:-phi-3-mini}
      SLM_PROVIDER: ${SLM_PROVIDER:-llama_cpp}
      ENABLE_SLM_FALLBACK: ${ENABLE_SLM_FALLBACK:-true}
      
      # Worker Settings
      MAX_LOAD: ${WORKER_MAX_LOAD:-5}
      TASK_TIMEOUT: ${TASK_TIMEOUT:-300}
      
      # Logging
      LOG_LEVEL: ${LOG_LEVEL:-INFO}
      LOG_FORMAT: ${LOG_FORMAT:-json}
      PYTHONPATH: /app
      PYTHONUNBUFFERED: 1
      ENVIRONMENT: ${ENVIRONMENT:-development}
    volumes:
      - ./common:/app/common:ro
      - ./ghosts:/app/ghosts:ro
      - ./mcp_server:/app/mcp_server:ro
      - ghost-logs:/app/logs
      - ghost-data:/app/data
    depends_on:
      ghost-base:
        condition: service_completed_successfully
      orchestrator:
        condition: service_healthy
    networks:
      - ghost-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "python", "-c", "import sys; sys.exit(0)"]
      interval: 30s
      timeout: 10s
      start_period: 60s
      retries: 3

  worker-2:
    build:
      context: .
      dockerfile: Dockerfiles/Dockerfile.worker
    container_name: ghost-worker-2
    ports:
      # Each worker exposes its own A2A port
      - "${WORKER_2_A2A_PORT:-8767}:8766"
    environment:
      # Agent Configuration (ADDED AGENT_ID)
      AGENT_ID: ${AGENT_ID:-worker-2}
      AGENT_ROLE: worker
      A2A_PORT: 8766
      WORKER_CAPABILITIES: ${WORKER_2_CAPABILITIES:-llm_inference,data_processing,analysis}
      
      # Redis for service discovery
      REDIS_URL: ${REDIS_URL:-redis://redis:6379}
      
      # Claude API (LLM)
      ANTHROPIC_API_KEY: ${ANTHROPIC_API_KEY}
      LLM_MODEL: ${LLM_MODEL:-claude-sonnet-4-5-20250929}
      
      # SLM Configuration (ADDED)
      SLM_ENDPOINT: ${SLM_ENDPOINT:-http://slm-server:8080}
      SLM_MODEL: ${SLM_MODEL:-phi-3-mini}
      SLM_PROVIDER: ${SLM_PROVIDER:-llama_cpp}
      ENABLE_SLM_FALLBACK: ${ENABLE_SLM_FALLBACK:-true}
      
      # Worker Settings
      MAX_LOAD: ${WORKER_MAX_LOAD:-5}
      TASK_TIMEOUT: ${TASK_TIMEOUT:-300}
      
      # Logging
      LOG_LEVEL: ${LOG_LEVEL:-INFO}
      LOG_FORMAT: ${LOG_FORMAT:-json}
      PYTHONPATH: /app
      PYTHONUNBUFFERED: 1
      ENVIRONMENT: ${ENVIRONMENT:-development}
    volumes:
      - ./common:/app/common:ro
      - ./ghosts:/app/ghosts:ro
      - ./mcp_server:/app/mcp_server:ro
      - ghost-logs:/app/logs
      - ghost-data:/app/data
    depends_on:
      ghost-base:
        condition: service_completed_successfully
      orchestrator:
        condition: service_healthy
    networks:
      - ghost-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "python", "-c", "import sys; sys.exit(0)"]
      interval: 30s
      timeout: 10s
      start_period: 60s
      retries: 3

  worker-3:
    build:
      context: .
      dockerfile: Dockerfiles/Dockerfile.worker
    container_name: ghost-worker-3
    ports:
      # Each worker exposes its own A2A port
      - "${WORKER_3_A2A_PORT:-8768}:8766"
    environment:
      # Agent Configuration (ADDED AGENT_ID)
      AGENT_ID: ${AGENT_ID:-worker-3}
      AGENT_ROLE: worker
      A2A_PORT: 8766
      WORKER_CAPABILITIES: ${WORKER_3_CAPABILITIES:-llm_inference,data_processing,analysis}
      
      # Redis for service discovery
      REDIS_URL: ${REDIS_URL:-redis://redis:6379}
      
      # Claude API (LLM)
      ANTHROPIC_API_KEY: ${ANTHROPIC_API_KEY}
      LLM_MODEL: ${LLM_MODEL:-claude-sonnet-4-5-20250929}
      
      # SLM Configuration (ADDED)
      SLM_ENDPOINT: ${SLM_ENDPOINT:-http://slm-server:8080}
      SLM_MODEL: ${SLM_MODEL:-phi-3-mini}
      SLM_PROVIDER: ${SLM_PROVIDER:-llama_cpp}
      ENABLE_SLM_FALLBACK: ${ENABLE_SLM_FALLBACK:-true}
      
      # Worker Settings
      MAX_LOAD: ${WORKER_MAX_LOAD:-5}
      TASK_TIMEOUT: ${TASK_TIMEOUT:-300}
      
      # Logging
      LOG_LEVEL: ${LOG_LEVEL:-INFO}
      LOG_FORMAT: ${LOG_FORMAT:-json}
      PYTHONPATH: /app
      PYTHONUNBUFFERED: 1
      ENVIRONMENT: ${ENVIRONMENT:-development}
    volumes:
      - ./common:/app/common:ro
      - ./ghosts:/app/ghosts:ro
      - ./mcp_server:/app/mcp_server:ro
      - ghost-logs:/app/logs
      - ghost-data:/app/data
    depends_on:
      ghost-base:
        condition: service_completed_successfully
      orchestrator:
        condition: service_healthy
    networks:
      - ghost-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "python", "-c", "import sys; sys.exit(0)"]
      interval: 30s
      timeout: 10s
      start_period: 60s
      retries: 3

# =============================================================================
# VOLUMES
# =============================================================================

volumes:
  # Application data (logs, cache, temporary files)
  ghost-data:
    driver: local

  # Logs from all services
  ghost-logs:
    driver: local

  # Redis persistence
  redis-data:
    driver: local
  
  # SLM model cache
  slm-cache:
    driver: local

# =============================================================================
# NETWORKS
# =============================================================================

networks:
  ghost-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16